---
output:
  word_document: default
  html_document: default
---
```{r}
#---------------------------- Data preparation ----------------
set.seed(123)

##### Data Load
house <- read.csv("c:/users/cyG770/downloads/House.csv",header = T)
house <- subset(house,select = LotFrontage:SalePrice)

#------------------------- Imputation ---------------------
#library(bnstruct)
#library(bitops)
library(randomForest)
house <- rfImpute(SalePrice ~ .,data=house)


##### Train and Test
pick <- sort(sample(ceiling(1:nrow(house)),nrow(house)/3))
train <- house[-pick,]
test <- house[pick,]

```

```{r}
### separated imputation-----------
set.seed(123)

##### Data Load
house <- read.csv("c:/users/cyG770/downloads/House.csv",header = T)
house <- subset(house,select = LotFrontage:SalePrice)

library(missForest)

##### Train and Test
pick <- sort(sample(ceiling(1:nrow(house)),nrow(house)/3))
train <- house[-pick,]
test <- house[pick,]


imp.feature.train = missForest(train)$ximp
imp.feature.test = missForest(test)$ximp[1:nrow(test),]

write.csv(imp.feature.train,"c:/users/cyg770/downloads/4001_part2/train_imp.csv")
write.csv(imp.feature.test,"c:/users/cyg770/downloads/4001_part2/test_imp.csv")

```

```{r}
#-----------------------------PLS----------------------------------
  set.seed(123)
  #dataset test
  is.imp <- TRUE 
  ####
  
  ###Transformation
  is.trans <- FALSE
  ###
  
  ########### Data Load
  if(is.imp){
    train <- read.csv("c:/users/cyG770/downloads/4001_part2/train_imp.csv",header = T)[,-1]
    test <- read.csv("c:/users/cyG770/downloads/4001_part2/test_imp.csv",header = T)[,-1]
  }else{
    train <- read.csv("c:/users/cyG770/downloads/4001_part2/train_house.csv",header = T)[,-1]
    test <- read.csv("c:/users/cyG770/downloads/4001_part2/test_house.csv",header = T)[,-1]
  }
  levels(test$LotShape) <- c('IR1', 'IR2', 'IR3', 'Reg')
  if(is.trans){
    train$SalePrice <- log(train$SalePrice)
    test$SalePrice <- log(test$SalePrice)
  }

  
############  Packages
library(pls)

############# fit
mod <- plsr(SalePrice~.,data=train, validation = "CV",segments=5)
summary(mod)
########### Determine number of components
validationplot(mod,val.type = "MSEP")
n <- selectNcomp(mod,method = "randomization")

#####final model
mod <- plsr(SalePrice~.,data=train,ncomp=n)
############# visualization
#plot(mod,plottype = "coef",ncomp=1:n, legendpos = "topleft")
coefplot(mod,ncomp = n,type='h')
#predplot(mod,newdata = test,which = 'test',line=T,col='blue',xlab='actual',main='prediction vs actual of testing set')

### pseudo R sq. of training data
1-mean((train$SalePrice-mod$fitted.values[,,n])^2)/var(train$SalePrice)



############# prediction
pred <- predict(mod, ncomp = n, newdata = test,type=c('response','scores'))
actual <- test$SalePrice

####residual plot.
res <- test$SalePrice-pred
std_pred_res <- (res-mean(res))/sd(res)

### predict vs actual plot
plot(pred~actual,col='blue',main='prediction vs actual of testing set',ylab='predicted')
abline(lm(pred~actual))

###residual diagnosis plot
hist(std_pred_res,breaks = 20,main='Std residual of prediction Histogram',xlab='std prediction residual',xlim = c(-4,5),probability = T)
lines(density(std_pred_res,adjust = 2),col='red',lwd=2)
plot(pred,std_pred_res, main='std residual vs predicted',xlab='predicted',ylab='std predicted residual')
lines(lowess(pred,std_pred_res),col='red')

### Bias, Variance, tested R sq./ MSE
mean((actual - mean(pred))^2) #bias
mean((mean(pred)-pred)^2) #variance
1-sum(res^2)/sum((actual-mean(actual))^2) #tested R sq.

```


```{r}
#-------------------------Random Forest (regression trees) ---------------------------
set.seed(123)
####### Packages
library(randomForest)
library(caret)  ## for random forest parameter tuning

is.imp <- FALSE
########### Data Load
if(is.imp){
  train <- read.csv("c:/users/cyG770/downloads/4001_part2/train_imp.csv",header = T)[,-1]
  test <- read.csv("c:/users/cyG770/downloads/4001_part2/test_imp.csv",header = T)[,-1]
}else{
  train <- read.csv("c:/users/cyG770/downloads/4001_part2/train_sta.csv",header = T)[,-1]
  test <- read.csv("c:/users/cyG770/downloads/4001_part2/test_sta.csv",header = T)[,-1]
}

levels(test$LotShape) <- c('IR1', 'IR2', 'IR3', 'Reg')

# ------------ parameter tuning --------
customRF <- list(type = "Regression", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

# train model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(1:(ncol(train)-1+2)), .ntree=c(100,500,1000, 1500, 2000, 2500))
custom <- train(SalePrice~., data=train, method=customRF, metric='RMSE', tuneGrid=tunegrid, trControl=control)
plot(custom)

####MSE matrix for report
#write.csv(matrix(custom$results[,3]^2,nrow=6),"c:/users/cyg770/downloads/rf_cv_error.csv")


###final model
parm <- custom$bestTune
mod <- randomForest(SalePrice~.,data=train,mtry=parm$mtry,ntree=parm$ntree)

#importance plot
varImpPlot(custom$finalModel,pch=10,main='Variables Importance')

### MSE of training data
1-mean((train$SalePrice-mod$predicted)^2)/var(train$SalePrice)


### prediction
pred <- predict(mod,newdata = test)
actual <- test$SalePrice

####predicted vs actual plot
plot(pred~actual,col='blue',main='prediction vs actual of testing set',ylab='predicted')
abline(lm(pred~actual))

#residuals
res <- actual-pred
std_pred_res <- (res-mean(res))/sd(res)

###residual diagnosis plot

hist(std_pred_res,breaks = 20,main='Std residual of prediction Histogram',xlab='std prediction residual',xlim = c(-4,5),probability = T)
lines(density(std_pred_res,adjust = 2),col='red',lwd=2)
plot(pred,std_pred_res, main='std residual vs predicted',xlab='predicted',ylab='std predicted residual')
lines(lowess(pred,std_pred_res),col='red')

### Bias, Variance, tested R sq./ MSE
mean((actual - mean(pred))^2) #bias
mean((mean(pred)-pred)^2) #variance
1-sum(res^2)/sum((actual-mean(actual))^2) #tested R sq.
```



```{r}
# ------------------------ Boosting ---------------------------------

set.seed(123)
####### Packages
library(gbm)
library(plyr)
library(caret)  ## for random forest parameter tuning

is.imp <- TRUE
########### Data Load
if(is.imp){
  train <- read.csv("c:/users/cyG770/downloads/4001_part2/train_imp.csv",header = T)[,-1]
  test <- read.csv("c:/users/cyG770/downloads/4001_part2/test_imp.csv",header = T)[,-1]
}else{
  train <- read.csv("c:/users/cyG770/downloads/4001_part2/train_house.csv",header = T)[,-1]
  test <- read.csv("c:/users/cyG770/downloads/4001_part2/test_house.csv",header = T)[,-1]
}
levels(test$LotShape) <- c('IR1', 'IR2', 'IR3', 'Reg')


# ------------ traing and tuning --------

##tree model
tuned <- train(
  SalePrice ~., data = train, method = "gbm",
  trControl = trainControl("cv", number = 5)
  )

plot(tuned)
parm <- tuned$bestTune

#### CV error
print('####CV error#####')
min(tuned$results$RMSE)
print('#############')

# ---------------- final model -----------
mod <- gbm(SalePrice~., data =train,
           n.trees = parm$n.trees,
           interaction.depth = parm$interaction.depth,
           shrinkage = parm$shrinkage,
           n.minobsinnode = parm$n.minobsinnode
           )

####relative influence plot
temp <- summary(mod)
barchart(var~rel.inf,data=temp,h=T,xlab = 'relative influence',main='Relative inf. of variables', col='blue')

### pseudo R sq. of training data
1-mean((train$SalePrice-mod$fit)^2)/var(train$SalePrice)


### prediction
pred <- predict(mod,newdata = test,n.trees = parm$n.trees)
actual <- test$SalePrice

####predicted vs actual plot
plot(pred~actual,col='blue',main='prediction vs actual of testing set',ylab='predicted')
abline(lm(pred~actual))

#residuals
res <- actual-pred
std_pred_res <- (res-mean(res))/sd(res)

###residual diagnosis plot
hist(std_pred_res,breaks = 20,main='Std residual of prediction Histogram',xlab='std prediction residual',xlim = c(-4,5),probability = T)
lines(density(std_pred_res,adjust = 2),col='red',lwd=2)
plot(pred,std_pred_res, main='std residual vs predicted',xlab='predicted',ylab='std predicted residual')
lines(lowess(pred,std_pred_res),col='red')

### Bias, Variance, tested R sq./ MSE
mean((actual - mean(pred))^2) #bias
mean((mean(pred)-pred)^2) #variance
1-sum(res^2)/sum((actual-mean(actual))^2) #tested R sq.
```